#### general settings
name: EHDRGAN # name里有debug也可以
model_type: EHDRGANModel  # 这里写你自定义的model名字,一定是在models里注册了的model
show_network: false
use_tb_logger: true  # tensor board logger
distortion: sr
scale: 1  # 这个会自动放到datasets里面
# gpu_ids: [0]
# dist:  分布式训练
num_gpu: auto
manual_seed: 0

#### datasets
datasets:
  train:
    name: Single_LDR2HDR
    type: LQGTDataset  # 这里写你注册了的数据集的名字
    data_type: img  # img或lmdb
    condition: image
    # 剩下的这些参数都必须和data/对应type的dataset类需要用到的重合
    # 存放gt、ratio、LQ images的文件夹
    dataroot_LQ: /Users/azusa/Documents/dataset/NTIRE2021_HDR/train_medium_sub
    dataroot_GT: /Users/azusa/Documents/dataset/NTIRE2021_HDR/train_gt_sub
    dataroot_ratio: /Users/azusa/Documents/dataset/NTIRE2021_HDR/train_ratio

    num_gpu: 0  # 是否有gpu
    prefetch_mode: cpu  # cpu or cuda

    # n_workers: 8
    # batch_size: 16
    # @params
    GT_size: 160  # the final input size will be [batch_size, 3, GT_size, GT_size]
    use_flip: true
    use_rot: true
    # 没看到的
    # phase: train
    # dist: false
    # sampler: None
    # seed : None
    # use_shuffle: true
    # num_prefetch_queue: ?
    num_worker_per_gpu: 8
    batch_size_per_gpu: 4  # 16
    dataset_enlarge_ratio: 1
  val:
     name: Single_LDR2HDR_val
     type: LQGTDataset
     data_type: img
     dataroot_LQ: /Users/azusa/Documents/dataset/NTIRE2021_HDR/val_medium_sub
     dataroot_GT: /Users/azusa/Documents/dataset/NTIRE2021_HDR/val_gt_sub
     dataroot_ratio: /Users/azusa/Documents/dataset/NTIRE2021_HDR/val_ratio
     condition: image

#### network structures
# generator
network_g:
  type: R2AttU_Net
  img_ch: 3
  output_ch: 3
  t: 4

# discriminator
network_d:
  type: R2AttU_Net
  img_ch: 3
  output_ch: 1
  t: 2

#### path
path:
  # root: ./
  # pretrain_model_g: ../experiments/...  # 如果有预训练的模型就往这里填
  # pretrain_model_d: ../experiments/...
  # strict_load: false
  # resume_state: ../experiments/...
  # experiment root
  experiments_root: ./experiments
  # if not in_train:
  # results_root: xxx
  # log: log文件想储存在的文件夹名称

#### training settings: learning rate scheme, loss
train:
  # lr_G: !!float 2e-4
  # lr_scheme: MultiStepLR # MultiStepLR | CosineAnnealingLR_Restart
  # beta1: 0.9
  # beta2: 0.99
  # niter: 1000000 # 600000
  total_iter: 8  #就是总迭代数，total epoch = total_iter / num_batch, 600000
  warmup_iter: -1  # no warm up

  # lr_scheme: MultiStepLR
  # lr_steps: [200000, 400000, 600000, 800000]
  # lr_gamma: 0.5
  scheduler:
    type: MultiStepLR
    milestones: [200000, 400000, 600000, 800000]
    gamma: 0.5

  # options for losses
  # pixel loss
  pixel_opt:
    type: L1Loss      # L1Loss | MSELoss | CharbonnierLoss | WeightedTVLoss | PerceptualLoss | GANLoss
    loss_weight: 1.0  # gradient_penalty_loss | r1_penalty | g_path_regularize'
    reduction: mean
  # perceptual loss
  perceptual_opt:
    type: PerceptualLoss
    layer_weights:
      # before relu
      'conv1_2': 0.1
      'conv2_2': 0.1
      'conv3_4': 1
      'conv4_4': 1
      'conv5_4': 1
    vgg_type: vgg19
    use_input_norm: true
    perceptual_weight: !!float 1.0
    style_weight: 0
    range_norm: false
    criterion: l1
  # gan loss
  gan_opt:
    type: GANLoss
    gan_type: vanilla
    real_label_val: 1.0
    fake_label_val: 0.0
    loss_weight: !!float 1e-1

  manual_seed: 10

  # net_d_iter: 1  discriminator的迭代数 意思是总iter迭代n次才会计算一次loss
  # net_d_init_iters: 0

  optim_g:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [ 0.9, 0.99 ]

  optim_d:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [ 0.9, 0.99 ]
#validation
val:
  # validation frequent
  val_freq: !!float 1.0   # !!float 5e3 = 存checkpoint的频率
  save_img: true
#### logger
logger:
  print_freq: 1  # 100
  save_checkpoint_freq: !!float 1.0 # !!float 5e3
  use_tb_logger: true  # 是否使用logger
  wandb:
    project: ~
    resume_id: ~


#### 没看到的
# auto_resume: false
# in_train: true