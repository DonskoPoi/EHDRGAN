#### general settings
name: HDRGAN_debug # name里有debug也可以
model_type: EHDRGANModel  # 这里写你自定义的model名字,一定是在models里注册了的model
use_tb_logger: true
distortion: sr
scale: 1  # 这个会自动放到datasets里面
# gpu_ids: [0]
num_gpu: auto
manual_seed: 0

#### datasets
datasets:
  train:
    name: Single_LDR2HDR
    type: LQGTDataset  # 这里写你注册了的数据集的名字
    data_type: img  # img或lmdb
    condition: image
    # 剩下的这些参数都必须和data/对应type的dataset类需要用到的重合
    # 存放gt、ratio、LQ images的文件夹
    dataroot_LQ: /Volumes/Azi/dataset/NTIRE2021_HDR/train_medium
    dataroot_GT: /Volumes/Azi/dataset/NTIRE2021_HDR/train_gt
    dataroot_ratio: /Volumes/Azi/dataset/NTIRE2021_HDR/train_ratio

    num_gpu: 0  # 是否有gpu
    prefetch_mode: cpu  # cpu or cuda

    # n_workers: 8
    # batch_size: 16
    # @params
    GT_size: 160
    use_flip: true
    use_rot: true
    # 没看到的
    # phase: train
    # dist: false
    # sampler: None
    # seed : None
    # use_shuffle: true
    # num_prefetch_queue: ?
    num_worker_per_gpu: 8
    batch_size_per_gpu: 16
    dataset_enlarge_ratio: 1
  # val:
    # name: Single_LDR2HDR
    # type: LQGT_condition
    # dataroot_LQ: /data0/NTIRE2021_HDR/000_Valid_SingleFrame_FirstStage/medium
    # dataroot_GT: /data0/NTIRE2021_HDR/000_Valid_SingleFrame_FirstStage/gt
    # dataroot_ratio: /data0/NTIRE2021_HDR/000_Valid_SingleFrame_FirstStage/alignratio
    # condition: image

#### network structures
# generator
network_g:
  type: R2AttU_Net
  img_ch: 3
  output_ch: 3
  t: 4

# discriminator
network_d:
  type: R2AttU_Net
  img_ch: 3
  output_ch: 1
  t: 2

#### path
path:
  # root: ./
  # pretrain_model_g: ../experiments/...  # 如果有预训练的模型就往这里填
  # pretrain_model_d: ../experiments/...
  # strict_load: false
  # resume_state: ../experiments/...
  # experiment root
  experiments_root: ./experiments
  # if not in_train:
  # results_root: xxx
  # log: log文件想储存在的文件夹名称

#### training settings: learning rate scheme, loss
train:
  # lr_G: !!float 2e-4
  # lr_scheme: MultiStepLR # MultiStepLR | CosineAnnealingLR_Restart
  # beta1: 0.9
  # beta2: 0.99
  # niter: 1000000 # 600000
  total_iter: 600000  #就是总迭代数，total epoch = total_iter / num_batch
  warmup_iter: -1  # no warm up

  # lr_scheme: MultiStepLR
  # lr_steps: [200000, 400000, 600000, 800000]
  # lr_gamma: 0.5
  scheduler:
    type: MultiStepLR
    milestones: [200000, 400000, 600000, 800000]
    gamma: 0.5

  pixel_criterion: tanh_l1 # l1 | l2 | tanh_l1 | tanh_l2
  pixel_weight: 1.0

  manual_seed: 10
  val_freq: !!float 5e3

  optim_g:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [ 0.9, 0.99 ]

  optim_d:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [ 0.9, 0.99 ]

#### logger
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true  # 是否使用logger
  wandb:
    project: ~
    resume_id: ~


#### 没看到的
auto_resume: false
in_train: true